# CalibrationXferViaKD
The repository contains the code for Calibration Transfer Via Knowledge Distillation. The paper is accepted as ORAL presentation in ACCV 2024
Modern deep neural networks often suffer from miscalibration, leading to overly confident errors that undermine their reliability. Although Knowledge Distillation (KD) is known to improve student classifier accuracy, its impact on model calibration remains unclear. It is generally assumed that well-calibrated teachers produce well-calibrated students. However, previous findings indicate that teachers calibrated with label smoothing (LS) result in less accurate students~\cite{shen2021label}. This paper explores the theoretical foundations of KD, revealing that prior results are artifacts of specific calibration methods rather than KD itself. Our study shows that calibrated teachers can effectively transfer calibration to their students, but not all training regimes are equally effective. Notably, teachers calibrated using dynamic label smoothing methods yield better-calibrated student classifiers through KD. We also show that transfer of calibration can be induced from lower capacity teachers to larger capacity students (aka rKD). The proposed KD based Calibration framework, named KDC, leads to a state-of-the-art (\SOTA) calibration results. 
